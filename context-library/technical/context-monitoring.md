# Context Monitoring: Token Usage Display Pattern

**Framework:** FnMCP.IvanTheGeekDevFramework  
**Purpose:** Display token usage after every Claude response  
**Updated:** 2025-11-11  
**Status:** Active - Required for all conversations

## The Pattern

**Display detailed token usage statistics at the end of EVERY response** to monitor context consumption and manage conversation flow efficiently.

## Required Format

```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä Context Usage: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 85,000 / 190,000 tokens (44.7%)

Allocation:
‚îú‚îÄ System Prompts:     ~5,000 tokens  (2.6%)
‚îú‚îÄ Project Knowledge:  ~2,000 tokens  (1.1%)
‚îú‚îÄ Conversation:      ~75,000 tokens (39.5%)
‚îÇ  ‚îú‚îÄ Your messages:  ~12,000 tokens
‚îÇ  ‚îú‚îÄ My responses:   ~58,000 tokens
‚îÇ  ‚îî‚îÄ Tool calls:      ~5,000 tokens
‚îî‚îÄ This Response:      ~3,000 tokens  (1.6%)

Remaining: 105,000 tokens (55.3%) ‚úì Comfortable

Status Legend: ‚úì Comfortable (0-75%) | ‚ö† Moderate (75-85%) | üî¥ High (85%+)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

## Visual Bar Calculation (CRITICAL)

**20 characters total, each block = 5%**

```python
# CORRECT calculation
total_tokens = 190000
used_tokens = 85000
percentage = (used_tokens / total_tokens) * 100  # 44.74%

# Calculate filled blocks
filled = int(percentage / 5)  # int(44.74 / 5) = int(8.948) = 8
empty = 20 - filled           # 20 - 8 = 12

bar = "‚ñà" * filled + "‚ñë" * empty
# Result: "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë" (8 filled, 12 empty)
```

**Rounding:** Use `int()` to truncate (floor). Do NOT round up.

**Common mistake:** Don't calculate `filled = (used_tokens / total_tokens) * 20` directly - this gives wrong results because you're not accounting for the percentage-to-blocks conversion.

### Verification Examples

| Used % | Blocks | Bar |
|--------|--------|-----|
| 5%     | 1      | `[‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]` |
| 25%    | 5      | `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]` |
| 42.9%  | 8      | `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]` |
| 50%    | 10     | `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]` |
| 75%    | 15     | `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë]` |
| 90%    | 18     | `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë]` |
| 100%   | 20     | `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]` |

## Components Explained

**Visual Bar:**
- 20 characters total
- `‚ñà` represents used tokens (filled)
- `‚ñë` represents remaining tokens (empty)
- Each character = 5% of total capacity

**Allocation Breakdown:**
- **System Prompts**: Claude's base instructions (~5K fixed)
- **Project Knowledge**: Files loaded at conversation start
- **Conversation**: All messages and tool results
  - Your messages: User input
  - My responses: Claude's replies
  - Tool calls: MCP operations, searches, etc.
- **This Response**: Current reply token cost

**Status Indicators:**
- `‚úì Comfortable` (0-75% used): Plenty of headroom
- `‚ö† Moderate` (75-85% used): Plan to wrap up soon
- `üî¥ High` (85%+ used): Finish current task, start new chat

## Implementation

### In Memory (Account-Level)
User preference stored in Claude's memory system:
> "Display token usage after EVERY response: line separators (‚îÅ), 20-char bar (‚ñà/‚ñë), tree allocation (‚îú‚îÄ), status legend (‚úì‚ö†üî¥). See technical/context-monitoring.md"

This ensures the pattern applies to **all conversations** (top-level and in projects).

### In Project Knowledge (Quick Start)
Framework overview includes complete format specification with calculation details.

## Why This Matters

### Without Monitoring
- Conversations hit limits unexpectedly
- Context loss mid-task
- Frustration and wasted time
- No visibility into token consumption patterns

### With Monitoring
- **Proactive management**: See limits approaching
- **Informed decisions**: Know when to continue vs. start fresh
- **Optimization feedback**: Measure impact of changes
- **Predictable workflow**: Plan conversation length

## Real-World Benefits

### Token Optimization Validation
```
Before MCP (loading full docs):
‚îú‚îÄ Project Knowledge: ~15,000 tokens (7.9%)

After MCP (Quick Start only):
‚îú‚îÄ Project Knowledge:  ~2,000 tokens (1.1%)

Savings: 13,000 tokens (87% reduction) ‚úì VERIFIED
```

### Session Planning
```
At 40% (76K used):
‚Üí Continue with complex features

At 70% (133K used):  
‚Üí Finish current task, prepare to wrap

At 85% (162K used):
‚Üí Complete work, start new chat immediately
```

### Debugging Aid
```
Unexpected 30K token response?
‚Üí Check breakdown to see why
‚Üí Adjust approach for efficiency
```

## Success Metrics

**Monitoring is working when:**
- Appears after every response consistently
- Shows accurate token counts
- **Visual bar correctly represents percentage** (8 blocks for ~40%, not 16!)
- Status indicators trigger at right thresholds
- User can plan conversation flow effectively
- Token optimizations are immediately visible

## Integration with "Enhance Nexus"

When running "enhance nexus", this monitoring:
1. Shows token cost of analysis
2. Tracks MCP update operations
3. Validates memory additions fit comfortably
4. Ensures updates don't bloat context

---

*Context monitoring transforms blind token consumption into visible, manageable resource usage - essential for efficient Nexus operation.*